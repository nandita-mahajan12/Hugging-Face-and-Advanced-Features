# -*- coding: utf-8 -*-
"""task6_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sOry1L0LKYIOQQBoB3S9XWbAPDcQpHvO
"""

!pip uninstall transformers -y
!pip uninstall tokenizers -y

!pip install --upgrade transformers datasets

import transformers
print(transformers.__version__)

pip install transformers datasets accelerate

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments

dataset = load_dataset("facebook/research-plan-gen", "ml")
print(dataset)
print(dataset["train"].column_names)

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

MAX_LEN = 384

def token(example):
  text = ("Goal:\n" + example["Goal"] + "\n\n" + "Reference Solution:\n" + example["Reference solution"])

  tokens = tokenizer(text, padding="max_length", truncation=True, max_length=MAX_LEN)

  tokens["labels"] = [
      t if t != tokenizer.pad_token_id else -100
      for t in tokens["input_ids"]
  ]

  return tokens

tokenized_dataset = dataset.map(token, remove_columns=dataset['train'].column_names)
tokenized_dataset

small_train = tokenized_dataset['train'].shuffle(seed=42).select(range(300))
small_eval = tokenized_dataset['test'].shuffle(seed=42).select(range(120))

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    weight_decay=0.01,
    logging_steps=50,
    fp16=False,
    report_to="none",
)

model = AutoModelForCausalLM.from_pretrained("distilgpt2")
model.resize_token_embeddings(len(tokenizer))

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_eval,
)

trainer.train()

model.save_pretrained("./result")
tokenizer.save_pretrained("./result")

trainer.state.epoch